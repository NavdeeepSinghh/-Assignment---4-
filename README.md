# ğŸ“Š Learning Probability Density Functions using GAN

## ğŸ§¾ Assignment Overview
This project focuses on learning an **unknown probability density function (PDF)** of a transformed random variable using a **Generative Adversarial Network (GAN)**.  
The distribution is learned **purely from data samples**, without assuming **any parametric form**.

---

## ğŸ”¢ Transformation Parameters ((a_r, b_r))

The transformation applied to the NOâ‚‚ concentration data (x) is:

**z = x + a_r sin(b_r x)**

The parameters are derived from the university roll number.

- ğŸ“ **Roll Number:** `102303773`

**a_r** = 0.5 Ã— (r mod 7)  
**b_r** = 0.3 Ã— ((r mod 5) + 1)

âœ… The values of **(a_r)** and **(b_r)** are **computed programmatically** in the code, ensuring **reproducibility** and **strict adherence** to the assignment guidelines.


---

## ğŸ§  GAN Architecture Description

A **vanilla GAN** is used to implicitly model the PDF of the transformed variable \(z\).

### ğŸ”¹ Generator
- **Input:** Gaussian noise \(\epsilon \sim \mathcal{N}(0,1)\)
- **Architecture:**  
  `1 â†’ 128 â†’ 128 â†’ 1`
- **Activations:** ReLU
- **Objective:** Generate samples that resemble the real transformed data

### ğŸ”¹ Discriminator
- **Input:** Real or generated samples of \(z\)
- **Architecture:**  
  `1 â†’ 128 â†’ 128 â†’ 1`
- **Activations:** LeakyReLU
- **Output:** Sigmoid
- **Objective:** Distinguish real samples from generated ones

âš ï¸ **No analytical or parametric probability distribution is assumed at any stage.**

---

## ğŸ“ˆ Learned PDF using GAN

After training:
- A large number of samples are generated using the **generator**
- The **probability density function** is estimated using **Kernel Density Estimation (KDE)**

The final comparison includes:
- ğŸ“Œ True PDF estimated from real transformed data  
- ğŸ“Œ Learned PDF estimated from GAN-generated samples  


### ğŸ“Š Final PDF Comparison Plot
**PDF Learning using GAN**

![PDF Learning using GAN](GAN_Estimated_pmf.png)


---

## ğŸ” Observations

### ğŸ§© Mode Coverage
The GAN successfully captures the **multi-modal structure** introduced by the sinusoidal transformation.  
All major modes present in the data distribution are represented, with **no evidence of mode collapse**.

### âš–ï¸ Training Stability
Training remains **stable** throughout the process.  
Generator and discriminator losses remain **bounded**, indicating a balanced adversarial learning process.

### ğŸ¯ Quality of Generated Distribution
The learned distribution closely approximates the **empirical distribution** of the transformed data.  
Minor differences in peak amplitudes are observed, which are expected due to adversarial training dynamics and KDE smoothing.

---

âœ¨ **This project demonstrates the effectiveness of GANs for implicit density learning when explicit probability models are unavailable.**
